{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd004f3ce0738d928d74413a2b10d0d4c487f39bbf2ffd0e3f43a6ab028b956cd75",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import autograd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def im2col(image, ksize, stride):\n",
    "    bs, channel, width, height = image.shape\n",
    "    res = image.unfold(2,ksize,stride).unfold(3,ksize,stride).swapaxes(1,5).swapaxes(1,3)\n",
    "    size = res.shape\n",
    "    return res.reshape(size[0], size[1] * size[2], size[3] * size[4] * size[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SConv2d(autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, inputS, weight, weightS, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "        # col_weights = weight.reshape(weight.shape[0], -1).swapaxes(0,1)\n",
    "        # input = F.pad(input,tuple(4*[padding]))\n",
    "        # bs, xc, xw, xh = input.shape\n",
    "        # oc, _, kw, kh = weight.shape\n",
    "        # ow, oh = xw - kw + 1, xh - kh + 1\n",
    "\n",
    "        # col_image = F.unfold(input,(kw,kh)).transpose(1,2)\n",
    "        # conv_out = col_image.matmul(w.view(w.size(0),-1).t()).transpose(1,2)\n",
    "        # conv_out = F.fold(conv_out, (ow, oh), (1,1))\n",
    "        # ctx.save_for_backward(col_image, weight, bias)\n",
    "        conv_out = F.conv2d(input, weight, bias, stride, padding, dilation, groups)\n",
    "        padded_input = F.pad(input,tuple(4*[padding]))\n",
    "        ctx.save_for_backward(padded_input, weight, bias, torch.IntTensor([padding], device=padded_input.device))\n",
    "        return conv_out, torch.ones_like(conv_out)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, grad_outputS):\n",
    "        input, weight, bias, padding = ctx.saved_tensors\n",
    "        col_image = F.unfold(input,(3,3)).transpose(1,2)\n",
    "        bs, channels, ow, oh = grad_output.shape\n",
    "        oc, ic, kw, kh = weight.shape\n",
    "        # col_grad_output = grad_output.view(bs, channels, -1)\n",
    "        grad_w = grad_output.view(bs, channels, -1).bmm(col_image).sum(dim=0).view(weight.shape)\n",
    "        grad_wS = grad_outputS.view(bs, channels, -1).bmm(col_image**2).sum(dim=0).view(weight.shape) # SSSS\n",
    "\n",
    "        if bias is None:\n",
    "            grad_b = None\n",
    "        else:\n",
    "            grad_b = grad_output.sum(axis=[0,2,3])\n",
    "\n",
    "        grad_output_padded = F.pad(grad_output,tuple(4*[kw-1-padding.item()]))\n",
    "        col_grad = F.unfold(grad_output_padded,(kh,kw)).transpose(1,2)\n",
    "        grad_outputS_padded = F.pad(grad_outputS,tuple(4*[kw-1-padding.item()])) # SSSS\n",
    "        col_gradS = F.unfold(grad_outputS_padded,(kh,kw)).transpose(1,2)\n",
    "        \n",
    "        flipped_w = weight.flip([2,3]).swapaxes(0,1)\n",
    "        col_flip = flipped_w.reshape(flipped_w.size(0),-1)\n",
    "        grad_i = col_grad.matmul(col_flip.t()).transpose(1,2)\n",
    "        grad_i = F.fold(grad_i, (ow, oh), (1,1))\n",
    "        grad_iS = col_gradS.matmul(col_flip.t()).transpose(1,2)\n",
    "        grad_iS = F.fold(grad_iS, (ow, oh), (1,1))\n",
    "\n",
    "        return grad_i, grad_iS, grad_w, grad_wS, grad_b, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(3,5,4,4).requires_grad_()\n",
    "imgS = torch.ones(3,5,4,4).requires_grad_()\n",
    "w = torch.randn(2,5,3,3).requires_grad_()\n",
    "wS = torch.ones(2,5,3,3).requires_grad_()\n",
    "bias = torch.zeros(2).requires_grad_()\n",
    "res1,_ = SConv2d.apply(img,imgS,w,wS,bias,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = copy.deepcopy(img.grad.data)\n",
    "T2 = copy.deepcopy(w.grad.data)\n",
    "T3 = copy.deepcopy(bias.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = F.conv2d(img,w,bias,1,1)\n",
    "img.grad.zero_()\n",
    "bias.grad.zero_()\n",
    "w.grad.zero_()\n",
    "res2.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(9.5367e-07)\ntensor(1.9073e-06)\ntensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print((img.grad - T1).abs().max())\n",
    "print((w.grad - T2).abs().max())\n",
    "print((bias.grad - T3).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[-0.0386, -0.6824, -0.5217, -0.6673],\n          [ 0.0108, -1.2389, -0.8433, -0.2332],\n          [-2.1736,  0.5494,  0.8251,  1.1906],\n          [ 0.2444, -1.0515,  1.6519, -1.5744]],\n\n         [[ 0.4461,  1.6695, -1.1383,  0.2146],\n          [ 1.6683, -0.1425, -0.4049,  1.1140],\n          [ 0.1828,  2.0047, -0.9005, -0.7964],\n          [ 0.5337,  1.0797,  0.0670,  1.3408]],\n\n         [[ 0.5395,  0.9447, -0.7407, -1.6365],\n          [ 0.9528,  0.5593, -0.6031,  0.2240],\n          [-2.2354, -0.6138, -1.0768,  1.3020],\n          [ 0.8636, -1.4690,  0.3154,  0.3527]]],\n\n\n        [[[ 0.5729,  0.7669,  1.2477, -1.4297],\n          [ 1.1332,  0.5756, -0.8515,  0.9711],\n          [ 0.6159, -2.3166,  0.8089,  1.0219],\n          [ 0.0121, -0.4188,  0.1152, -0.1013]],\n\n         [[ 0.4683, -1.1388,  0.2252,  1.2950],\n          [-0.0072,  0.7322, -0.5640, -0.4931],\n          [ 0.9604, -0.4985,  0.1336,  0.2823],\n          [-1.5844, -1.2147,  1.1050, -1.0086]],\n\n         [[ 1.8520,  0.1835,  0.9110, -0.4171],\n          [-0.7989, -0.4720, -0.7759, -1.2783],\n          [-0.9623,  0.2145,  0.6461,  0.8290],\n          [ 0.1191, -0.6871,  0.7064, -0.4999]]]])\n"
     ]
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2,return_indices=True)\n",
    "a = torch.randn(2,3,4,4)\n",
    "r, i = pool(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[ 0.0108, -0.2332],\n          [ 0.5494,  1.6519]],\n\n         [[ 1.6695,  1.1140],\n          [ 2.0047,  1.3408]],\n\n         [[ 0.9528,  0.2240],\n          [ 0.8636,  1.3020]]],\n\n\n        [[[ 1.1332,  1.2477],\n          [ 0.6159,  1.0219]],\n\n         [[ 0.7322,  1.2950],\n          [ 0.9604,  1.1050]],\n\n         [[ 1.8520,  0.9110],\n          [ 0.2145,  0.8290]]]])\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[ 4,  7],\n          [ 9, 14]],\n\n         [[ 1,  7],\n          [ 9, 15]],\n\n         [[ 4,  7],\n          [12, 11]]],\n\n\n        [[[ 4,  2],\n          [ 8, 11]],\n\n         [[ 5,  3],\n          [ 8, 14]],\n\n         [[ 0,  2],\n          [ 9, 11]]]])\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-1.2389, -0.5217])\n"
     ]
    }
   ],
   "source": [
    "print(a.view(2,1,-1)[torch.LongTensor([0,0]),torch.LongTensor([0,0]),torch.LongTensor([5,2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 4,  7,  9, 14,  1,  7,  9, 15,  4,  7, 12, 11]],\n",
       "\n",
       "        [[ 4,  2,  8, 11,  5,  3,  8, 14,  0,  2,  9, 11]]])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "i.view(2,1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0108, -0.2332],\n",
       "          [ 0.5494,  1.6519]],\n",
       "\n",
       "         [[ 1.6695,  1.1140],\n",
       "          [ 2.0047,  1.3408]],\n",
       "\n",
       "         [[ 0.9528,  0.2240],\n",
       "          [ 0.8636,  1.3020]]],\n",
       "\n",
       "\n",
       "        [[[ 1.1332,  1.2477],\n",
       "          [ 0.6159,  1.0219]],\n",
       "\n",
       "         [[ 0.7322,  1.2950],\n",
       "          [ 0.9604,  1.1050]],\n",
       "\n",
       "         [[ 1.8520,  0.9110],\n",
       "          [ 0.2145,  0.8290]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "def parse_indice(indice):\n",
    "    bs, ch, w, h = indice.shape\n",
    "    length = w * h\n",
    "    BD = torch.LongTensor(list(range(bs))).expand([length,ch,bs]).swapaxes(0,2).reshape(-1)\n",
    "    CD = torch.LongTensor(list(range(ch))).expand([bs,length,ch]).swapaxes(1,2).reshape(-1)\n",
    "    return [BD, CD, indice.view(-1)]\n",
    "\n",
    "T = parse_indice(i)\n",
    "a.view([2,3,-1])[T].view(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 1, 1, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "(1,1) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}