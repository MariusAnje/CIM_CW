{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from models import SCrossEntropyLoss, SMLP3, SMLP4, SLeNet, CIFAR, FakeSCrossEntropyLoss\n",
    "import modules\n",
    "from qmodels import QSLeNet, QCIFAR\n",
    "import resnet\n",
    "import qresnet\n",
    "import qvgg\n",
    "import qdensnet\n",
    "from modules import SModule\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "from cw_attack import Attack\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def CEval():\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    model.clear_noise()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # images = images.view(-1, 784)\n",
    "            outputs = model(images)\n",
    "            if len(outputs) == 2:\n",
    "                outputs = outputs[0]\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            correction = predictions == labels\n",
    "            correct += correction.sum()\n",
    "            total += len(correction)\n",
    "    return (correct/total).cpu().numpy()\n",
    "\n",
    "def NEval(dev_var, write_var):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    model.clear_noise()\n",
    "    with torch.no_grad():\n",
    "        model.set_noise(dev_var, write_var)\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # images = images.view(-1, 784)\n",
    "            outputs = model(images)\n",
    "            if len(outputs) == 2:\n",
    "                outputs = outputs[0]\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            correction = predictions == labels\n",
    "            correct += correction.sum()\n",
    "            total += len(correction)\n",
    "    return (correct/total).cpu().numpy()\n",
    "\n",
    "def NEachEval(dev_var, write_var):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    model.clear_noise()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            model.clear_noise()\n",
    "            model.set_noise(dev_var, write_var)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # images = images.view(-1, 784)\n",
    "            outputs = model(images)\n",
    "            if len(outputs) == 2:\n",
    "                outputs = outputs[0]\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            correction = predictions == labels\n",
    "            correct += correction.sum()\n",
    "            total += len(correction)\n",
    "    return (correct/total).cpu().numpy()\n",
    "\n",
    "def NTrain(epochs, header, dev_var=0.0, write_var=0.0, verbose=False):\n",
    "    best_acc = 0.0\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.\n",
    "        # for images, labels in tqdm(trainloader):\n",
    "        for images, labels in trainloader:\n",
    "            model.clear_noise()\n",
    "            model.set_noise(dev_var, write_var)\n",
    "            optimizer.zero_grad()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # images = images.view(-1, 784)\n",
    "            outputs = model(images)\n",
    "            loss = criteriaF(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        # test_acc = NEachEval(dev_var, write_var)\n",
    "        test_acc = CEval()\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.state_dict(), f\"tmp_best_{header}.pt\")\n",
    "        if verbose:\n",
    "            print(f\"epoch: {i:-3d}, test acc: {test_acc:.4f}, loss: {running_loss / len(trainloader):.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "def str2bool(a):\n",
    "    if a == \"True\":\n",
    "        return True\n",
    "    elif a == \"False\":\n",
    "        return False\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{a}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(alpha=1000000.0, calc_S=True, dev_var=0.3, device='cuda:0', div=1, header=1, layerwise=False, mask_p=0.01, method='SM', model='MLP4', model_path='./pretrained', noise_epoch=100, pretrained=True, save_file=True, train_epoch=20, train_var=0.0, use_mask=True, verbose=False, write_var=0.03)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train_epoch', action='store', type=int, default=20,\n",
    "        help='# of epochs of training')\n",
    "parser.add_argument('--noise_epoch', action='store', type=int, default=100,\n",
    "        help='# of epochs of noise validations')\n",
    "parser.add_argument('--train_var', action='store', type=float, default=0.0,\n",
    "        help='device variation [std] when training')\n",
    "parser.add_argument('--dev_var', action='store', type=float, default=0.3,\n",
    "        help='device variation [std] before write and verify')\n",
    "parser.add_argument('--write_var', action='store', type=float, default=0.03,\n",
    "        help='device variation [std] after write and verify')\n",
    "parser.add_argument('--mask_p', action='store', type=float, default=0.01,\n",
    "        help='portion of the mask')\n",
    "parser.add_argument('--device', action='store', default=\"cuda:0\",\n",
    "        help='device used')\n",
    "parser.add_argument('--verbose', action='store', type=str2bool, default=False,\n",
    "        help='see training process')\n",
    "parser.add_argument('--model', action='store', default=\"MLP4\", choices=[\"MLP3\", \"MLP4\", \"LeNet\", \"CIFAR\", \"Res18\", \"TIN\", \"QLeNet\", \"QCIFAR\", \"QRes18\", \"QDENSE\", \"QTIN\", \"QVGG\"],\n",
    "        help='model to use')\n",
    "parser.add_argument('--method', action='store', default=\"SM\", choices=[\"second\", \"magnitude\", \"saliency\", \"random\", \"SM\"],\n",
    "        help='method used to calculate saliency')\n",
    "parser.add_argument('--alpha', action='store', type=float, default=1e6,\n",
    "        help='weight used in saliency - substract')\n",
    "parser.add_argument('--header', action='store',type=int, default=1,\n",
    "        help='use which saved state dict')\n",
    "parser.add_argument('--pretrained', action='store',type=str2bool, default=True,\n",
    "        help='if to use pretrained model')\n",
    "parser.add_argument('--use_mask', action='store',type=str2bool, default=True,\n",
    "        help='if to do the masking experiment')\n",
    "parser.add_argument('--model_path', action='store', default=\"./pretrained\",\n",
    "        help='where you put the pretrained model')\n",
    "parser.add_argument('--save_file', action='store',type=str2bool, default=True,\n",
    "        help='if to save the files')\n",
    "parser.add_argument('--calc_S', action='store',type=str2bool, default=True,\n",
    "        help='if calculated S grad if not necessary')\n",
    "parser.add_argument('--div', action='store', type=int, default=1,\n",
    "        help='division points for second')\n",
    "parser.add_argument('--layerwise', action='store',type=str2bool, default=False,\n",
    "        help='if do it layer by layer')\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "print(args)\n",
    "header = time.time()\n",
    "header_timer = header\n",
    "parent_path = \"./\"\n",
    "\n",
    "device = torch.device(args.device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BS = 128\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='~/Private/data', train=True,\n",
    "                                        download=False, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BS,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.MNIST(root='~/Private/data', train=False,\n",
    "                                    download=False, transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BS,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "model = SLeNet()\n",
    "# model = QSLeNet()\n",
    "\n",
    "model.to(device)\n",
    "model.push_S_device()\n",
    "model.clear_noise()\n",
    "model.clear_mask()\n",
    "criteria = SCrossEntropyLoss()\n",
    "criteriaF = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.train_epoch = 20\n",
    "# args.dev_var = 0.3\n",
    "# args.train_var = 0.0\n",
    "# args.verbose = True\n",
    "\n",
    "# model.to_first_only()\n",
    "# NTrain(args.train_epoch, header, args.train_var, 0.0, args.verbose)\n",
    "# if args.train_var > 0:\n",
    "#     state_dict = torch.load(f\"tmp_best_{header}.pt\")\n",
    "#     model.load_state_dict(state_dict)\n",
    "# model.from_first_back_second()\n",
    "# torch.save(model.state_dict(), f\"saved_B_{header}.pt\")\n",
    "# print(f\"No mask no noise: {CEval():.4f}\")\n",
    "\n",
    "# state_dict = torch.load(f\"saved_B_{header}.pt\")\n",
    "# model.load_state_dict(state_dict)\n",
    "# model.clear_mask()\n",
    "\n",
    "# performance = NEachEval(args.dev_var, 0.0)\n",
    "# print(f\"No mask noise acc: {performance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No mask no noise: 0.9902\n"
     ]
    }
   ],
   "source": [
    "header = 1\n",
    "state_dict = torch.load(f\"saved_B_{header}.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.clear_mask()\n",
    "print(f\"No mask no noise: {CEval():.4f}\")\n",
    "model.to_first_only()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WCW(Attack):\n",
    "    def __init__(self, model, c=1e-4, kappa=0, steps=1000, lr=0.01):\n",
    "        super().__init__(\"CW\", model)\n",
    "        self.model = model\n",
    "        self.c = c\n",
    "        self.kappa = kappa\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "        self._supported_mode = ['default', 'targeted']\n",
    "    \n",
    "    def get_noise(self):\n",
    "        w = []\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, modules.NModule) or isinstance(m, modules.SModule):\n",
    "                m.noise.requires_grad_()\n",
    "                w.append(m.noise)\n",
    "        return w\n",
    "    \n",
    "    def noise_l2(self):\n",
    "        size = 0\n",
    "        w = 0\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, modules.NModule) or isinstance(m, modules.SModule):\n",
    "                w += m.noise.pow(2).sum().item()\n",
    "                size += len(m.noise.view(-1))\n",
    "        return w / size\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        images = images.clone().detach().to(self.device)\n",
    "        labels = labels.clone().detach().to(self.device)\n",
    "\n",
    "        if self._targeted:\n",
    "            target_labels = self._get_target_label(images, labels)\n",
    "\n",
    "        best_adv_images = images.clone().detach()\n",
    "        best_L2 = 1e10*torch.ones((len(images))).to(self.device)\n",
    "        prev_cost = 1e100\n",
    "        dim = len(images.shape)\n",
    "\n",
    "        MSELoss = nn.MSELoss(reduction='none')\n",
    "        Flatten = nn.Flatten()\n",
    "        w = self.get_noise()\n",
    "        current_L2 = self.noise_l2()\n",
    "        \n",
    "\n",
    "        optimizer = optim.Adam(w, lr=self.lr, weight_decay=self.c)\n",
    "\n",
    "        for step in range(self.steps):\n",
    "\n",
    "            outputs = self.model(images)\n",
    "            if self._targeted:\n",
    "                f_loss = self.f(outputs, target_labels).sum()\n",
    "            else:\n",
    "                f_loss = self.f(outputs, labels).sum()\n",
    "\n",
    "            cost = self.c*f_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update adversarial images\n",
    "            # _, pre = torch.max(outputs.detach(), 1)\n",
    "            # correct = (pre == labels).float()\n",
    "\n",
    "            # # filter out images that get either correct predictions or non-decreasing loss, \n",
    "            # # i.e., only images that are both misclassified and loss-decreasing are left \n",
    "            # mask = (1-correct)*(best_L2 > current_L2.detach())\n",
    "            # best_L2 = mask*current_L2.detach() + (1-mask)*best_L2\n",
    "\n",
    "            # mask = mask.view([-1]+[1]*(dim-1))\n",
    "            # best_adv_images = mask*adv_images.detach() + (1-mask)*best_adv_images\n",
    "\n",
    "            # # Early stop when loss does not converge.\n",
    "            # # max(.,1) To prevent MODULO BY ZERO error in the next step.\n",
    "            # if step % max(self.steps//10,1) == 0:\n",
    "            #     if cost.item() > prev_cost:\n",
    "            #         return best_adv_images\n",
    "            #     prev_cost = cost.item()\n",
    "\n",
    "        # return best_adv_images\n",
    "\n",
    "    def tanh_space(self, x):\n",
    "        return 1/2*(torch.tanh(x) + 1)\n",
    "\n",
    "    def inverse_tanh_space(self, x):\n",
    "        # torch.atanh is only for torch >= 1.7.0\n",
    "        return self.atanh(x*2-1)\n",
    "\n",
    "    def atanh(self, x):\n",
    "        return 0.5*torch.log((1+x)/(1-x))\n",
    "\n",
    "    # f-function in the paper\n",
    "    def f(self, outputs, labels):\n",
    "        one_hot_labels = torch.eye(len(outputs[0]))[labels].to(self.device)\n",
    "\n",
    "        i, _ = torch.max((1-one_hot_labels)*outputs, dim=1) # get the second largest logit\n",
    "        j = torch.masked_select(outputs, one_hot_labels.bool()) # get the largest logit\n",
    "\n",
    "        if self._targeted:\n",
    "            return torch.clamp((i-j), min=-self.kappa)\n",
    "        else:\n",
    "            return torch.clamp((j-i), min=-self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack mode is changed to 'targeted(random).'\n"
     ]
    }
   ],
   "source": [
    "model.clear_noise()\n",
    "attacker = WCW(model, c=1, kappa=0, steps=1000, lr=5e-5)\n",
    "attacker.set_mode_targeted_random(n_classses=10)\n",
    "images, labels = iter(testloader).next()\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "adv_examples = attacker(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08332809060811996\n",
      "0.00023530688487545786\n"
     ]
    }
   ],
   "source": [
    "w = attacker.get_noise()\n",
    "m = []\n",
    "for mm in w:\n",
    "    m.append(mm.max().item())\n",
    "print(max(m))\n",
    "print(attacker.noise_l2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1172, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "predicted = model(images).argmax(dim=1)\n",
    "print((predicted == labels).sum() / len(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = adv_examples[18].cpu().numpy().reshape(28,28)\n",
    "o = images[18].cpu().numpy().reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x282c8388048>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(a, cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(o, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04f3ce0738d928d74413a2b10d0d4c487f39bbf2ffd0e3f43a6ab028b956cd75"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
